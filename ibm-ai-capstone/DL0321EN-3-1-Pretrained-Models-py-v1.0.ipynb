{"cells":[{"cell_type":"markdown","id":"77887c9d-c2ff-4dd4-b2c6-64bfa1984219","metadata":{},"source":["<a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\"> </a>\n","\n","<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"]},{"cell_type":"markdown","id":"13b30d7e-8769-497a-8ff3-410157a58567","metadata":{},"source":["## Objective\n"]},{"cell_type":"markdown","id":"373dcde3-aeb9-47f2-95b4-832192acc8de","metadata":{},"source":["In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"]},{"cell_type":"markdown","id":"cc030d9b-7972-4688-86b1-205951d1163b","metadata":{},"source":["## Table of Contents\n","\n","<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n","\n","<font size = 3> \n","    \n","1. <a href=\"#item31\">Import Libraries and Packages</a>\n","2. <a href=\"#item32\">Download Data</a>  \n","3. <a href=\"#item33\">Define Global Constants</a>  \n","4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n","5. <a href=\"#item35\">Compile and Fit Model</a>\n","\n","</font>\n","    \n","</div>\n"]},{"cell_type":"markdown","id":"6773ce55-dd0f-49b4-aa00-0c6d6d4e10a8","metadata":{},"source":["   \n"]},{"cell_type":"markdown","id":"9d762fa9-3236-411c-af87-b8de53f576a8","metadata":{},"source":["<a id='item31'></a>\n"]},{"cell_type":"markdown","id":"1da2417d-cf35-4426-933e-0f94ab2704b1","metadata":{},"source":["## Import Libraries and Packages\n"]},{"cell_type":"markdown","id":"86e78465-733f-43e0-8404-fe3539697b0e","metadata":{},"source":["Let's start the lab by importing the libraries that we will be using in this lab. First we will need the library that helps us to import the data.\n"]},{"cell_type":"markdown","id":"df933b81-9331-4f83-a691-f4000da40400","metadata":{},"source":["First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"]},{"cell_type":"code","execution_count":16,"id":"ae3d4906-1ca0-4da4-8e9d-0318c7cf80f3","metadata":{},"outputs":[],"source":["from keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"markdown","id":"feb9d8f2-3a9b-41fe-a4bd-2eb9c0a762f3","metadata":{},"source":["In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"]},{"cell_type":"code","execution_count":2,"id":"4ac3c0af-d7d2-4b86-b360-f43231471500","metadata":{},"outputs":[],"source":["import keras\n","from keras.models import Sequential\n","from keras.layers import Dense"]},{"cell_type":"markdown","id":"feef4bee-e4f2-4ecd-9677-1208dce1700b","metadata":{},"source":["Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"]},{"cell_type":"code","execution_count":8,"id":"0c1537cb-03a6-4f04-b906-ab9ed76d26ee","metadata":{},"outputs":[],"source":["from tensorflow.keras.applications.resnet50 import preprocess_input, ResNet50"]},{"cell_type":"markdown","id":"9cb5437b-c5e6-4967-8de1-c03c746e2ece","metadata":{},"source":["<a id='item32'></a>\n"]},{"cell_type":"markdown","id":"a5819d9d-af9c-429b-914c-64eb76e60ce8","metadata":{},"source":["## Download Data\n"]},{"cell_type":"markdown","id":"0d995f70-5dfb-47dd-a116-a11c3235ba8f","metadata":{},"source":["In this section, you are going to download the data from IBM object storage using **skillsnetwork.prepare** command. skillsnetwork.prepare is a command that's used to download a zip file, unzip it and store it in a specified directory.\n"]},{"cell_type":"raw","id":"b0ae1a20-2062-4f1b-96ce-6e0674975622","metadata":{},"source":["## get the data\n","await skillsnetwork.prepare(\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip\", overwrite=True)"]},{"cell_type":"markdown","id":"1adf0e50-909c-4bdb-89d9-c773a89216ed","metadata":{},"source":["Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks.\n"]},{"cell_type":"markdown","id":"9d9d9beb-9dd8-4e83-9ca9-9e2c09f19eab","metadata":{},"source":["**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50** error. So please **DO NOT DO IT**.\n"]},{"cell_type":"markdown","id":"366d2205-be27-4b90-82cc-3dd130fd5fd9","metadata":{},"source":["<a id='item33'></a>\n"]},{"cell_type":"markdown","id":"a4533766-4ae0-432e-b6cd-e32b558fadf0","metadata":{},"source":["## Define Global Constants\n"]},{"cell_type":"markdown","id":"acfc926b-9b40-4ead-81aa-b6f4e3dbfd88","metadata":{},"source":["Here, we will define constants that we will be using throughout the rest of the lab. \n","\n","1. We are obviously dealing with two classes, so *num_classes* is 2. \n","2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n","3. We will training and validating the model using batches of 100 images.\n"]},{"cell_type":"code","execution_count":9,"id":"eadf578c-16b2-425a-885e-1f5202becc12","metadata":{},"outputs":[],"source":["num_classes = 2\n","\n","image_resize = 224\n","\n","batch_size_training = 100\n","batch_size_validation = 100"]},{"cell_type":"markdown","id":"ddea71c8-954b-4a80-9664-dcaa0d589811","metadata":{},"source":["<a id='item34'></a>\n"]},{"cell_type":"markdown","id":"7a4a47fa-9090-4e6e-8c78-d98b03cfc38d","metadata":{},"source":["## Construct ImageDataGenerator Instances\n"]},{"cell_type":"markdown","id":"352ef6bc-0d38-4949-808a-a37b0b051871","metadata":{},"source":["In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"]},{"cell_type":"code","execution_count":10,"id":"72a935a2-9f94-42e6-b764-e01de7ac1af9","metadata":{},"outputs":[],"source":["data_generator = ImageDataGenerator(\n","    preprocessing_function=preprocess_input,\n",")"]},{"cell_type":"markdown","id":"0e7df7c6-d10f-466d-bc7b-738eca43d839","metadata":{},"source":["Next, we will use the *flow_from_directory* method to get the training images as follows:\n"]},{"cell_type":"code","execution_count":12,"id":"14dd3fdb-bc4e-4af5-8c3b-24292f73dcdf","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 30001 images belonging to 2 classes.\n"]}],"source":["train_generator = data_generator.flow_from_directory(\n","    '../concrete_data_week3/train',\n","    target_size=(image_resize, image_resize),\n","    batch_size=batch_size_training,\n","    class_mode='categorical')"]},{"cell_type":"markdown","id":"60d98b33-d357-4a11-ad61-63055ba10334","metadata":{},"source":["**Note**: in this lab, we will be using the full data-set of 40,000 images for training and validation.\n"]},{"cell_type":"markdown","id":"e4537a5e-adcf-4722-95c0-9b277b445bd6","metadata":{},"source":["**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"]},{"cell_type":"code","execution_count":15,"id":"3d4b940f-2dc4-44f0-b26e-a04906e0bcfb","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 10001 images belonging to 2 classes.\n"]}],"source":["validation_generator = data_generator.flow_from_directory(\n","    '../concrete_data_week3/valid',\n","    target_size=(image_resize, image_resize),\n","    batch_size=batch_size_validation,\n","    class_mode='categorical')"]},{"cell_type":"markdown","id":"83c9dbef-2907-497a-9d4a-4ad2828a563f","metadata":{},"source":["Double-click __here__ for the solution.\n","<!-- The correct answer is:\n","validation_generator = data_generator.flow_from_directory(\n","    'concrete_data_week3/valid',\n","    target_size=(image_resize, image_resize),\n","    batch_size=batch_size_validation,\n","    class_mode='categorical')\n","-->\n","\n"]},{"cell_type":"markdown","id":"c5afab31-6a54-43a2-ae3f-6a85e9c5ffe8","metadata":{},"source":["<a id='item35'></a>\n"]},{"cell_type":"markdown","id":"c20de955-f27d-461c-8c71-9b0189014f71","metadata":{},"source":["## Build, Compile and Fit Model\n"]},{"cell_type":"markdown","id":"ade32f00-7084-47c7-8b46-6350be8350bb","metadata":{},"source":["In this section, we will start building our model. We will use the Sequential model class from Keras.\n"]},{"cell_type":"code","execution_count":17,"id":"e6e60cc3-76f1-4096-ab2f-2ea30f49700c","metadata":{},"outputs":[],"source":["model = Sequential()"]},{"cell_type":"markdown","id":"0fdc30dd-813a-4b2c-b23d-e48b0f3b5530","metadata":{},"source":["Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"]},{"cell_type":"code","execution_count":18,"id":"391d3165-5d15-407d-bb54-9262b92ddffd","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n","94765736/94765736 [==============================] - 28s 0us/step\n"]}],"source":["model.add(ResNet50(\n","    include_top=False,\n","    pooling='avg',\n","    weights='imagenet',\n","    ))"]},{"cell_type":"markdown","id":"382b4886-4d8b-4451-baca-462437fb18e6","metadata":{},"source":["Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"]},{"cell_type":"code","execution_count":19,"id":"3d6f3e8a-b880-44cb-96f6-aaa53a23b987","metadata":{},"outputs":[],"source":["model.add(Dense(num_classes, activation='softmax'))"]},{"cell_type":"markdown","id":"79deafb4-323b-4cc6-80e8-97a9d8d3703e","metadata":{},"source":["You can access the model's layers using the *layers* attribute of our model object. \n"]},{"cell_type":"code","execution_count":20,"id":"f7a33afa-9d85-47e6-8746-5bf8a6378fae","metadata":{},"outputs":[{"data":{"text/plain":["[<keras.engine.functional.Functional at 0x1ddd4099f90>,\n"," <keras.layers.core.dense.Dense at 0x1ddd3729090>]"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["model.layers"]},{"cell_type":"markdown","id":"934150ad-a32e-4d67-99cb-e3c8c6f63292","metadata":{},"source":["You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"]},{"cell_type":"markdown","id":"483de117-cda0-4b1e-bf37-18871f0c8977","metadata":{},"source":["You can access the ResNet50 layers by running the following:\n"]},{"cell_type":"code","execution_count":21,"id":"93ad07b7-a873-421d-a90f-9f6a8f1644d8","metadata":{},"outputs":[{"data":{"text/plain":["[<keras.engine.input_layer.InputLayer at 0x1ddd37284c0>,\n"," <keras.layers.reshaping.zero_padding2d.ZeroPadding2D at 0x1ddd37290c0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd375a710>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3728250>,\n"," <keras.layers.core.activation.Activation at 0x1ddd3723490>,\n"," <keras.layers.reshaping.zero_padding2d.ZeroPadding2D at 0x1ddd375a1a0>,\n"," <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x1ddd375b8e0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd375d600>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd375e5f0>,\n"," <keras.layers.core.activation.Activation at 0x1ddd375d810>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd375f400>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd375f2b0>,\n"," <keras.layers.core.activation.Activation at 0x1ddd375f640>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3759e70>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3771660>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3759bd0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3772080>,\n"," <keras.layers.merging.add.Add at 0x1ddd3770550>,\n"," <keras.layers.core.activation.Activation at 0x1ddd37d2dd0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd37d30d0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd37d5750>,\n"," <keras.layers.core.activation.Activation at 0x1ddd37d6260>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd37d6860>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd37d4940>,\n"," <keras.layers.core.activation.Activation at 0x1ddd38aea10>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd38af0a0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd38adf60>,\n"," <keras.layers.merging.add.Add at 0x1ddd38adc60>,\n"," <keras.layers.core.activation.Activation at 0x1ddd3770850>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd37d3df0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd38ae200>,\n"," <keras.layers.core.activation.Activation at 0x1ddd37225f0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd37d3ca0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3eec580>,\n"," <keras.layers.core.activation.Activation at 0x1ddd3eeeb00>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3eef190>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3eefb80>,\n"," <keras.layers.merging.add.Add at 0x1ddd3eef2b0>,\n"," <keras.layers.core.activation.Activation at 0x1ddd3ef9720>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3efbb80>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3efb970>,\n"," <keras.layers.core.activation.Activation at 0x1ddd3f11420>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3f11cc0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3f12e90>,\n"," <keras.layers.core.activation.Activation at 0x1ddd3f13d30>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3efa050>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3f12620>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3efae30>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3f29330>,\n"," <keras.layers.merging.add.Add at 0x1ddd3f12fe0>,\n"," <keras.layers.core.activation.Activation at 0x1ddd3f2a980>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3f2b2b0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3f2b7f0>,\n"," <keras.layers.core.activation.Activation at 0x1ddd3f2bd60>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3f3d420>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3f3e0e0>,\n"," <keras.layers.core.activation.Activation at 0x1ddd3f3efb0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3f3f640>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3f3d330>,\n"," <keras.layers.merging.add.Add at 0x1ddd3f3c250>,\n"," <keras.layers.core.activation.Activation at 0x1ddd3f59a20>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3f5a500>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3f3fd90>,\n"," <keras.layers.core.activation.Activation at 0x1ddd3f3d990>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3f29780>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3f10610>,\n"," <keras.layers.core.activation.Activation at 0x1ddd3f29000>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3efbc40>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd375c640>,\n"," <keras.layers.merging.add.Add at 0x1ddd3ef9150>,\n"," <keras.layers.core.activation.Activation at 0x1ddd3f5beb0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3f597b0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3f5be80>,\n"," <keras.layers.core.activation.Activation at 0x1ddd3f5bd60>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3f71270>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3f72b60>,\n"," <keras.layers.core.activation.Activation at 0x1ddd3f721a0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3f73460>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3f73f10>,\n"," <keras.layers.merging.add.Add at 0x1ddd3f71330>,\n"," <keras.layers.core.activation.Activation at 0x1ddd3f79780>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3f7ba60>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3f7bd90>,\n"," <keras.layers.core.activation.Activation at 0x1ddd3f7ab90>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3f8cdf0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3f8f130>,\n"," <keras.layers.core.activation.Activation at 0x1ddd3f8ed40>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3f7aa70>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3f8f9d0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3f7a0b0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3f8e9e0>,\n"," <keras.layers.merging.add.Add at 0x1ddd3f8fa90>,\n"," <keras.layers.core.activation.Activation at 0x1ddd3fa1d50>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3fa3040>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3fa2680>,\n"," <keras.layers.core.activation.Activation at 0x1ddd3fa3fd0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3fa30a0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3fa3160>,\n"," <keras.layers.core.activation.Activation at 0x1ddd3fb5ff0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3fa3340>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3fa1420>,\n"," <keras.layers.merging.add.Add at 0x1ddd3fa26e0>,\n"," <keras.layers.core.activation.Activation at 0x1ddd3f5a350>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3eec7f0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3f28af0>,\n"," <keras.layers.core.activation.Activation at 0x1ddd3f10550>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3fb5d50>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3fb7c70>,\n"," <keras.layers.core.activation.Activation at 0x1ddd3fb7eb0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3fcd8a0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3fce740>,\n"," <keras.layers.merging.add.Add at 0x1ddd3fceb30>,\n"," <keras.layers.core.activation.Activation at 0x1ddd3fcfd90>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3fdc700>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3fdd4e0>,\n"," <keras.layers.core.activation.Activation at 0x1ddd3fcfc40>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3fde320>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3fdf4f0>,\n"," <keras.layers.core.activation.Activation at 0x1ddd3fdf640>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3fe86d0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3fe9960>,\n"," <keras.layers.merging.add.Add at 0x1ddd3fdfd90>,\n"," <keras.layers.core.activation.Activation at 0x1ddd3feafb0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3feb8e0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3febe20>,\n"," <keras.layers.core.activation.Activation at 0x1ddd3feae90>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd4001450>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd4002d10>,\n"," <keras.layers.core.activation.Activation at 0x1ddd4002440>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd4000d60>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd4002980>,\n"," <keras.layers.merging.add.Add at 0x1ddd4002530>,\n"," <keras.layers.core.activation.Activation at 0x1ddd4018580>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd401ac20>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd401ba00>,\n"," <keras.layers.core.activation.Activation at 0x1ddd401a740>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd401bfd0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd40187c0>,\n"," <keras.layers.core.activation.Activation at 0x1ddd4019780>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd4002ef0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3feb550>,\n"," <keras.layers.merging.add.Add at 0x1ddd40199f0>,\n"," <keras.layers.core.activation.Activation at 0x1ddd3ef81c0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3f2b550>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3fcf6a0>,\n"," <keras.layers.core.activation.Activation at 0x1ddd3eeec50>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd3fcf430>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd402f7f0>,\n"," <keras.layers.core.activation.Activation at 0x1ddd402f3a0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd4019360>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd402eef0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd3f723b0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd402fd60>,\n"," <keras.layers.merging.add.Add at 0x1ddd402e020>,\n"," <keras.layers.core.activation.Activation at 0x1ddd402ec50>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd4041e40>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd4043820>,\n"," <keras.layers.core.activation.Activation at 0x1ddd40413f0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd4043340>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd4042770>,\n"," <keras.layers.core.activation.Activation at 0x1ddd4043460>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd405dcf0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd405ed40>,\n"," <keras.layers.merging.add.Add at 0x1ddd405dd80>,\n"," <keras.layers.core.activation.Activation at 0x1ddd405ea70>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd405fa00>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd405ea40>,\n"," <keras.layers.core.activation.Activation at 0x1ddd405e440>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd405dea0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd4082980>,\n"," <keras.layers.core.activation.Activation at 0x1ddd4082b90>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x1ddd4083580>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1ddd40824d0>,\n"," <keras.layers.merging.add.Add at 0x1ddd4083640>,\n"," <keras.layers.core.activation.Activation at 0x1ddd4083610>,\n"," <keras.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D at 0x1ddd40836d0>]"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["model.layers[0].layers"]},{"cell_type":"markdown","id":"c9b50534-db82-48ff-a966-db9c9db20b83","metadata":{},"source":["Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"]},{"cell_type":"code","execution_count":22,"id":"e9ab99d5-d322-41e7-9c67-7da3d02c0b81","metadata":{},"outputs":[],"source":["model.layers[0].trainable = False"]},{"cell_type":"markdown","id":"936f1a04-0bcc-4a29-935e-e20d23cd76cd","metadata":{},"source":["And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"]},{"cell_type":"code","execution_count":23,"id":"80e16337-42ed-4ee6-ab2c-3fa90719d4ff","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," resnet50 (Functional)       (None, 2048)              23587712  \n","                                                                 \n"," dense (Dense)               (None, 2)                 4098      \n","                                                                 \n","=================================================================\n","Total params: 23,591,810\n","Trainable params: 4,098\n","Non-trainable params: 23,587,712\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","id":"31c39eb0-b6c6-4f14-9c26-b861e3d071a0","metadata":{},"source":["Next we compile our model using the **adam** optimizer.\n"]},{"cell_type":"code","execution_count":24,"id":"4b9d8738-412b-4b18-afec-ff07dd316960","metadata":{},"outputs":[],"source":["model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"markdown","id":"74173ec3-81e4-41cb-84f0-047d562b195c","metadata":{},"source":["Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"]},{"cell_type":"code","execution_count":25,"id":"d0f49844-0ce6-468a-adc0-17c2d9ddbab5","metadata":{},"outputs":[],"source":["steps_per_epoch_training = len(train_generator)\n","steps_per_epoch_validation = len(validation_generator)\n","num_epochs = 2"]},{"cell_type":"markdown","id":"7f09403d-0dd1-492b-8630-86fb412f6f5a","metadata":{},"source":["Finally, we are ready to start training our model. Unlike a conventional deep learning training where data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"]},{"cell_type":"code","execution_count":26,"id":"0b0dc574-8803-44d7-b3d7-57026a9f74c0","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\Hush\\AppData\\Local\\Temp\\ipykernel_20756\\251737888.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  fit_history = model.fit_generator(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001DDD31ADC60> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: module 'gast' has no attribute 'Constant'\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001DDD31ADC60> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: module 'gast' has no attribute 'Constant'\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","301/301 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9900WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001DDD58B0820> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: module 'gast' has no attribute 'Constant'\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001DDD58B0820> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: module 'gast' has no attribute 'Constant'\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","301/301 [==============================] - 984s 3s/step - loss: 0.0279 - accuracy: 0.9900 - val_loss: 0.0069 - val_accuracy: 0.9987\n","Epoch 2/2\n","301/301 [==============================] - 967s 3s/step - loss: 0.0052 - accuracy: 0.9989 - val_loss: 0.0050 - val_accuracy: 0.9989\n"]}],"source":["fit_history = model.fit_generator(\n","    train_generator,\n","    steps_per_epoch=steps_per_epoch_training,\n","    epochs=num_epochs,\n","    validation_data=validation_generator,\n","    validation_steps=steps_per_epoch_validation,\n","    verbose=1,\n",")"]},{"cell_type":"markdown","id":"8ddfe7fa-64dc-446e-90f1-4caf95352890","metadata":{},"source":["Now that the model is trained, you are ready to start using it to classify images.\n"]},{"cell_type":"markdown","id":"acff630c-cf29-45b0-be4a-9fbcfb475b42","metadata":{},"source":["Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"]},{"cell_type":"code","execution_count":27,"id":"0424beb7-0c3f-4bb3-af40-14e852aaa92f","metadata":{},"outputs":[],"source":["model.save('classifier_resnet_model.h5')"]},{"cell_type":"markdown","id":"d02a8457-6201-4efd-9f84-ac65812ba037","metadata":{},"source":["Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"]},{"cell_type":"markdown","id":"2ebb1e58-fe8f-409e-afc8-b4c9dfc97672","metadata":{},"source":["### Thank you for completing this lab!\n","\n","This notebook was created by Alex Aklson. I hope you found this lab interesting and educational.\n"]},{"cell_type":"markdown","id":"d42d3248-8125-42c9-9a61-4b142ef12e35","metadata":{},"source":["This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1).\n"]},{"cell_type":"markdown","id":"1df79a67-08ce-40dd-9ee4-233b19da1060","metadata":{},"source":["\n","## Change Log\n","\n","|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","|---|---|---|---|\n","| 2020-09-18  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n","| 2023-01-03  | 3.0  | Artem |  Updated the file import section|\n","\n"]},{"cell_type":"markdown","id":"feb23059-dd19-4d06-b9b7-def2386544a6","metadata":{},"source":["<hr>\n","\n","Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_medium=dswb&utm_source=bducopyrightlink&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01).\n"]}],"metadata":{"kernelspec":{"display_name":"py10","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":4}
